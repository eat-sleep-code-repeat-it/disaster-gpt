
#  DisasterGPT 

Ask Anything About U.S. Disaster Declarations.

## How to run 
```bash
# Python 3.13.7

python -m venv venv
venv\Scripts\activate 

pip install gradio
pip install openai
pip install faiss-cpu
pip install python-dotenv
pip freeze > requirements.txt


# set OPENAI_API_KEY in .env file
pip install -r requirements.txt
python -m app.main
python -m app.main --no-verify-ssl


pip install pytest
pytest tests/
pytest tests/test_answer_eval.py
pytest tests/test_embedding_utils.py
pytest tests/test_rag_pipeline.py
```

## Sample Prompts
```js
is there an active disaster in Washington County, Oregon? YES & Evaluation
is there an disaster in Riverside, California?  YES & Evaluation
is there an active disaster in Riverside, California? NO & Evaluation
active fire disasters? YES,  Evaluation
give all fire disasters? NO,  Evaluation & Guardrail
```

## What we Get:
- A clean chat UI (like ChatGPT)
- Each message sends a user query
- RAG pipeline is run
    - create index and save it to local vector file
    - or load index from the saved vector file
- Assistant returns a structured answer using openai
- Built with Guardrail and evaluation
- Previous messages are preserved in context

## Project Structure
- Keeps Gradio app logic clean and isolated
- Separates data/model logic from UI
- Makes it easier to maintain, extend, or even deploy later (e.g., with FastAPI or Streamlit)

```bash
disaster-gpt/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py               # ðŸ”¹ Main Gradio app (entry point)
â”‚   â”œâ”€â”€ rag_pipeline.py       # ðŸ”¹ RAG logic (retrieval + answer generation) 
â”‚   â”œâ”€â”€ models.py             # ðŸ”¹ Pydantic models (e.g., DisasterDeclaration)
â”‚   â”œâ”€â”€ embedding_utils.py    # ðŸ”¹ Embedding + FAISS index handling
â”‚   â”œâ”€â”€ answer_eval.py        # ðŸ”¹ Guardrails & GPT-based evaluation
â”‚   â”œâ”€â”€ data_loader.py        # ðŸ”¹ Load/parse CSV data
â”‚   â””â”€â”€ constants.py          # ðŸ”¹ Paths, constants, config keys
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ disaster_declarations.csv  # ðŸ”¹ Source dataset
â”‚
â”œâ”€â”€ saved_index/              # ðŸ”¹ Store FAISS index & metadata
â”‚   â”œâ”€â”€ disaster_faiss.index
â”‚   â””â”€â”€ disaster_metadata.pkl
â”‚
â”œâ”€â”€ assets/                   # ðŸ”¹ (Optional) Images, logos, docs
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_rag_pipeline.py  # ðŸ”¹ Unit tests (pytest)
â”‚
â”œâ”€â”€ .env                      # ðŸ”¹ OpenAI keys, etc.
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt

app/main.py	Launches the Gradio UI (e.g., gr.ChatInterface)
app/rag_pipeline.py	Contains rag_pipeline() and chat_rag_fn() logic
app/models.py	Pydantic DisasterDeclaration and other data models
app/embedding_utils.py	Embedding + FAISS build/save/load
app/answer_eval.py	GPT-based evaluation and keyword-based guardrails
data/	Static data source (e.g., CSVs)
saved_index/	Stores generated FAISS index and metadata
tests/	Optional test suite using pytest or unittest
```

## tests
```bash
Disaster retrieval	search_similar_declarations()
Active disaster filtering	is_active_disaster()
Answer generation	generate_openai_answer()
Guardrails	validate_answer()
Full RAG pipeline test	chat_rag_fn()
OpenAI mocking	@patch(...)
```

## Future

Adapt retrieval + context + prompt formatting pipeline to conform to the MCP standard (or parts of it), you can plug into an MCP-compatible server or client.

### Scenario 1: You want to expose RAG pipeline as an MCP-compatible server
```js
Wrap retrieval and response code into an MCP-compatible handler (e.g., using LangChain's MCP tools)
Expose a REST or HTTP API using FastAPI or Flask
Follow the MCP schema for request/response (context, query, history, etc.)
ðŸ§± Example: Using LangChainâ€™s langchain-mcp
```

### Scenario 2: You want to consume an existing MCP server
```js
Letâ€™s say you want to delegate RAG to an MCP server, not implement one.
Format queries using the MCP request schema (user_input, retrieval_context, history, etc.)
Send that to the MCP server endpoint (e.g., via requests.post())
Receive the MCP-formatted answer
Display in Gradio app
```

### To integrate MCP into the current Gradio RAG app
1. Define MCP request/response format
2. Modify chat_rag_fn() to use MCP
3. (Optional) Expose an MCP server
```bash
Use MCP server for answers	Send query, history, and context to MCP API
Make my own MCP server	    Wrap my RAG logic in a FastAPI MCP route
Match MCP schema	        Use user_input, context, history fields
```


